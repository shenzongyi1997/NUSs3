{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Yq1ghQYALVy"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "**ASSIGNMENT DEADLINE: Week 7, 29 Sep 2020 17:00**\n",
    "\n",
    "In this assignemnt, the task is to implement some basic components for training neural network. You need to:\n",
    "\n",
    "- implement the [Adamax](https://ruder.io/optimizing-gradient-descent/index.html#adamax) algorithm in nn/optimizers.py\n",
    "- implement linear class in nn/operators.py, which are used by the Linear layers (in nn/layer.py) \n",
    "- implement leaky_relu class in nn/operators.py\n",
    "- implement a simple classifier in model/simple_classifier.py and train it to classify mnist images for digit 0 and 1.\n",
    "\n",
    "**Attention**:\n",
    "- To run this Jupyter notebook, you need to install the dependent libraries as stated in [README.MD](README.MD). You do not need and should not use other libraries (like tensorflow and pytorch) in your code except Python and numpy. The major version of Python should be 3.\n",
    "- You do not need a GPU for this assignment. CPU is enough.\n",
    "- Do not run this notebook before you finish the implementation of the required functions. otherwise, you will see errors as this notebook will call the functions to be implemented by you.\n",
    "- Do not change the signature (the name and arguments) of the existing functions in the repository ; otherwise your implementation cannot be tested correctly and you will get penalty.\n",
    "- Do not change the structure of files in the repository (e.g., adding, renaming or deleting any files); otherwise your implementation cannot be tested correctly and you will get penalty.\n",
    "- You can add functions in the existing files, but you should not change the import statements (e.g., adding a new import statement). For example, if you want to implement a function foo(), you can implement it inside operator.py and call it; but you cannot implement it in another file and import that file in operators.py. Otherwise your implementation cannot be tested correctly and you will get penalty.\n",
    "- After you implement one function, remember to restart the notebook kernel to help it recognize your fresh code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98RyzrTwALV0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_PxpeHpVALV_"
   },
   "source": [
    "## Structure of the repository\n",
    "\n",
    "The structure of this repository is shown as below:\n",
    "\n",
    "```bash\n",
    "codes/\n",
    "    nn/              # components of neural networks\n",
    "        operators.py    # operators; **You need to edit this file to add missing code**\n",
    "        optimizers.py   # optimizing methods; **You need to implement the Adamax algorithm**\n",
    "        layers.py       # layer abstract\n",
    "        loss.py         # loss function for optimization\n",
    "        initializers.py # initializing methods to initialize parameters (like weights, bias)\n",
    "    model/\n",
    "        simple_classifier.py  ## a simple model with two fully connected linear layer\n",
    "    utils/              # some additional tools\n",
    "        check_grads_cnn.py  # help you check your forward function and backward function\n",
    "        tools.py        # other useful functions for testing the codes\n",
    "        dataloader.py    ## loading data\n",
    "    main.ipynb          # this notebook which calls the functions in other modules/files\n",
    "    README.MD           # list of dependent libraries\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0dKjUQlALWE"
   },
   "source": [
    "## Functionality of this notebook\n",
    "\n",
    "This iPython notebook serves to:\n",
    "\n",
    "- explain code structure, main APIs\n",
    "- explain your implementation task and tuning task\n",
    "- provide code to test your implemented forward and backward function for different operations\n",
    "- provide related materials to help you understand the implementation of some operations and optimizers\n",
    "\n",
    "*You can type `jupyter lab` in the terminal to start this jupyter notebook when your current working directory is cs5242. It's much more convinient than jupyter notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYLckkOVALWH",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Your tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLlTTp2vALWI"
   },
   "source": [
    "## Base classes\n",
    "\n",
    "In [nn/optimizers.py](nn/optimizers.py), we define the base optimizer class. We have also implemented SGD, Adagrad, Adam for you. **You only need to implement the AdaMax optimizer in the [nn/optimizers.py](nn/optimizers.py) following the same style.**\n",
    "\n",
    "```python\n",
    "class Optimizer():\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        \"\"\"Initialization\n",
    "\n",
    "        # Arguments\n",
    "            lr: float, learnig rate \n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, x, x_grad, iteration):\n",
    "        \"\"\"Update parameters with gradients\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sheduler(self, func, iteration):\n",
    "        \"\"\"learning rate sheduler, to change learning rate with respect to iteration\n",
    "\n",
    "        # Arguments\n",
    "            func: function, arguments are lr and iteration\n",
    "            iteration: int, current iteration number in the whole training process (not in that epoch)\n",
    "\n",
    "        # Returns\n",
    "            lr: float, the new learning rate\n",
    "        \"\"\"\n",
    "        lr = func(self.lr, iteration)\n",
    "        return lr\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "In [nn/operators.py](nn/operators.py), we define the base operator class and have implemented some operations (like relu) for you. **You only need to implement the rest operations in the [nn/operators.py](nn/operators.py) following the same style.\n",
    "\n",
    "```python\n",
    "class operator(object):\n",
    "    \"\"\"\n",
    "    operator abstraction\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward operation, reture output\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        \"\"\"Backward operation, return gradient to input\"\"\"\n",
    "        raise NotImplementedError\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTMwwArIALWJ"
   },
   "source": [
    "## Adamax Optimizer\n",
    "In the file [nn/optimizers.py](nn/optimizers.py), there are 6 types of optimizer (`SGD`, `Adam`, `RMSprop`, `Adamax`, `Nadam`and `Adagrad`). **You only need to implement the `update` function of `Adamax`**. Follow https://ruder.io/optimizing-gradient-descent/index.html#adamax for implementing `Adamax`.\n",
    "\n",
    "`Adamax` optimizer is initialized like this:\n",
    "\n",
    "```python\n",
    "class Adamax(Optimizer):\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, sheduler_func=None):\n",
    "        \"\"\"Initialization\n",
    "\n",
    "        # Arguments\n",
    "            lr: float, learnig rate \n",
    "            beta_1: float\n",
    "            beta_2: float\n",
    "            epsilon: float, precision to avoid numerical error \n",
    "            decay: float, the learning rate decay ratio\n",
    "        \"\"\"\n",
    "        super(Adamax, self).__init__(lr)\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        if not self.epsilon:\n",
    "            self.epsilon = 1e-8\n",
    "        self.momentum = None\n",
    "        self.accumulators = None\n",
    "        self.sheduler_func = sheduler_func\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3frv_DIALWL"
   },
   "source": [
    "### Forward function of AdaMax Optimizer\n",
    "\n",
    "You need to implement the update function of the `Adamax` class in [nn/operators.py](nn/operators.py).\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJO09F66ALWM",
    "outputId": "180ba9bf-68db-4fe3-c3c8-2b288675f1ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "import tensorflow!\n",
      "import keras!\n",
      "import optimizers!\n",
      "import Adamax!\n",
      "finish importing Adamax!\n",
      "import utils!\n",
      "finish import utils!\n",
      "start!\n",
      "WARNING:tensorflow:Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "key: layer1  value: [[ 0.2742197  -0.421238  ]\n",
      " [-0.04879677  0.37199432]\n",
      " [ 0.04655409 -0.07221776]\n",
      " [ 0.20866805  0.38413125]\n",
      " [-0.12011111  0.31888682]\n",
      " [-0.13493294  0.1065194 ]\n",
      " [ 0.23235255 -0.10372084]\n",
      " [ 0.05149519  0.3167202 ]\n",
      " [-0.26066342  0.6803432 ]\n",
      " [-0.06371373 -0.15783554]]\n",
      "(10, 2)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4947e9cf8704>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mupdate_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madamax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mkeras_update_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\work\\CS5242\\CS5242 Assignment1\\nn\\optimizers.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, w, w_grads, iteration)\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_correction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#### import keras\n",
    "print(\"import tensorflow!\")\n",
    "import tensorflow as tf\n",
    "print(\"import keras!\")\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "print(\"import optimizers!\")\n",
    "from keras.optimizers import Adamax as keras_Adamax\n",
    "print(\"import Adamax!\")\n",
    "from nn.optimizers import Adamax\n",
    "print(\"finish importing Adamax!\")\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "import warnings\n",
    "print(\"import utils!\")\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils.tools import rel_error\n",
    "print(\"finish import utils!\")\n",
    "\n",
    "batch_size = 20\n",
    "in_features = 10\n",
    "out_features = 2\n",
    "x = np.random.uniform(size=(batch_size, in_features))\n",
    "label = np.random.randint(low = 0, high = out_features, size=batch_size)\n",
    "weight = {}\n",
    "grad = {}\n",
    "adamax = Adamax(lr=0.001, beta_1=0.9, beta_2=0.999,epsilon=1e-07)\n",
    "layer1 = Dense(out_features, input_shape = (in_features,))\n",
    "\n",
    "loss_fn = SparseCategoricalCrossentropy()\n",
    "print(\"start!\")\n",
    "\n",
    "optimizer = keras_Adamax(lr=0.001, beta_1=0.9, beta_2=0.999,epsilon=1e-07)\n",
    "with tf.GradientTape() as tape:\n",
    "\n",
    "    logits = layer1(x)\n",
    "    loss = loss_fn(label, logits)\n",
    "    gradients = tape.gradient(loss, layer1.trainable_weights)\n",
    "    \n",
    "    weight['layer1'] = layer1.trainable_weights[0].numpy()\n",
    "    grad['layer1'] = gradients[0].numpy()\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, layer1.trainable_weights))\n",
    "    update_weight = adamax.update(weight,grad,0)['layer1']\n",
    "    keras_update_weight = layer1.trainable_weights[0].numpy()\n",
    "\n",
    "print('Relative error of Adamax Update (<1e-6 will be fine): ', rel_error(keras_update_weight, update_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vW3G_NvjALWX"
   },
   "source": [
    "## Linear layer\n",
    "\n",
    "Linear (in [nn/layers.py](nn/layers.py)) implements the fully connected linear layer. It maintains the weight matrix and bias vector, and calls the forward and backward funtion of `linear` class in [nn/operators.py](nn/operators.py) to do the real operations. \n",
    "\n",
    "\n",
    "```python\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_features, out_features, name='linear', initializer=Gaussian()):\n",
    "        \"\"\"Initialization\n",
    "\n",
    "        # Arguments\n",
    "            in_features: int, the number of input features\n",
    "            out_features: int, the numbet of required output features\n",
    "            initializer: Initializer class, to initialize weights\n",
    "        \"\"\"\n",
    "        super(Linear, self).__init__(name=name)\n",
    "        self.linear = linear()\n",
    "\n",
    "        self.trainable = True\n",
    "\n",
    "        self.weights = initializer.initialize((in_features, out_features))\n",
    "        self.bias = np.zeros(out_features)\n",
    "\n",
    "        self.w_grad = np.zeros(self.weights.shape)\n",
    "        self.b_grad = np.zeros(self.bias.shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.linear.forward(input, self.weights, self.bias)\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        in_grad, self.w_grad, self.b_grad = self.linear.backward(\n",
    "            out_grad, input, self.weights, self.bias)\n",
    "        return in_grad\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSMgCnu1ALWY"
   },
   "source": [
    "### Forward function of linear operator\n",
    "\n",
    "You need to implement the forward function of the `linear` class in [nn/operators.py](nn/operators.py).\n",
    "\n",
    "The input consists of N data points, each with in_features channels, Output consists of N data points with out_features channels.\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_uD69MEALWZ",
    "outputId": "85a2f382-d325-4c3c-d414-5d1a051df8ce"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nn.layers import Linear\n",
    "from utils.tools import rel_error\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "batch_size = 10\n",
    "in_features = 3\n",
    "out_features = 10\n",
    "input = np.random.uniform(size=(batch_size, in_features))\n",
    "\n",
    "linear = Linear(in_features, out_features)\n",
    "out = linear.forward(input)\n",
    "\n",
    "keras_linear = Sequential()\n",
    "layer1 = Dense(out_features, input_shape = (in_features,))\n",
    "keras_linear.add(layer1) \n",
    "keras_out = keras_linear.predict(input, batch_size=batch_size) ##specify the shape of weight matrix\n",
    "keras_linear.layers[0].set_weights([linear.weights, linear.bias])\n",
    "keras_out = keras_linear.predict(input, batch_size=batch_size)\n",
    "\n",
    "print('Relative error (<1e-6 will be fine): ', rel_error(out, keras_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhQofmEjALWh"
   },
   "source": [
    "### Backward function of linear operator\n",
    " \n",
    "You need to implement the backward function for the `linear` class in the file [nn/operators.py](nn/operators.py). \n",
    "\n",
    "When you are done, restart jupyter notebook and run the following to check your backward implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85ltDwEIALWi",
    "outputId": "0571d061-406b-4eba-8171-75c5fdfbe74d"
   },
   "outputs": [],
   "source": [
    "from nn.layers import Linear\n",
    "import numpy as np\n",
    "from utils.check_grads_cnn import check_grads_layer\n",
    "\n",
    "batch = 10\n",
    "in_features = 3\n",
    "out_features = 10\n",
    "\n",
    "input = np.random.uniform(size=(batch, in_features))\n",
    "out_grad = np.random.uniform(size=(batch, out_features))\n",
    "\n",
    "linear = Linear(in_features, out_features)\n",
    "\n",
    "check_grads_layer(linear, input, out_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw7xzpxiALWp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3igE-WaIALWw"
   },
   "source": [
    "## Leaky_ReLU\n",
    "\n",
    "Leaky_ReLU (in nn/layer.py) implements leaky relu. It calls the forward and backward funtion of the leaky_relu class in [nn/operatiors.py](nn/operators.py) to do the real operations. \n",
    "\n",
    "The initialization, forward and backward funtion of the `Leaky_ReLU` layer are shown as below:\n",
    "\n",
    "```python\n",
    "class Leaky_ReLU(Layer):\n",
    "    def __init__(self, alpha = 0.01, name='leaky_relu'):\n",
    "        \"\"\"Initialization\n",
    "        \"\"\"\n",
    "        super(Leaky_ReLU, self).__init__(name=name)\n",
    "        # alpha: Float >= 0. Negative slope coefficient. Default to 0.01.\n",
    "        self.leaky_relu = leaky_relu(alpha)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        # Arguments\n",
    "            input: numpy array\n",
    "\n",
    "        # Returns\n",
    "            output: numpy array\n",
    "        \"\"\"\n",
    "        output = self.leaky_relu.forward(input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        \"\"\"Backward pass\n",
    "\n",
    "        # Arguments\n",
    "            out_grad: numpy array, gradient to output\n",
    "            input: numpy array, same with forward input\n",
    "\n",
    "        # Returns\n",
    "            in_grad: numpy array, gradient to input \n",
    "        \"\"\"\n",
    "        in_grad = self.leaky_relu.backward(out_grad, input)\n",
    "        return in_grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8iajCUEALWx"
   },
   "source": [
    "### Forward function of leaky-relu operator\n",
    "\n",
    "You need to implement the forward function for `leaky_relu` class in the file [nn/operators.py](nn/operators.py).\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YX7VWVNALWy",
    "outputId": "0de4dd79-d01c-41d9-f30a-f64baa6c1a11"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nn.layers import Leaky_ReLU\n",
    "from keras.layers import LeakyReLU\n",
    "from utils.tools import rel_error\n",
    "\n",
    "batch_size = 10\n",
    "in_features = 10\n",
    "alpha = 0.01\n",
    "input = np.random.uniform(low=-1.0, high=1.0,size=(batch_size, in_features))\n",
    "leaky_relu = Leaky_ReLU(alpha)\n",
    "keras_leaky_relu = LeakyReLU(alpha)\n",
    "out = leaky_relu.forward(input)\n",
    "keras_out = keras_leaky_relu(input)\n",
    "\n",
    "print('Relative error of Leaky ReLU (<1e-6 will be fine): ', rel_error(out, keras_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dy_SzlBAALW3"
   },
   "source": [
    "### Backward function of leaky_relu operator\n",
    "\n",
    "In [nn/operatiors.py](nn/operators.py), you need to implement the backward function for `leaky_relu` class. After the implementation, restart jupyter notebook and run the following cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4ibcWhnALW4",
    "outputId": "d5ff75e5-1e65-424f-f281-2b2b764ba2f7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.check_grads_cnn import check_grads_layer\n",
    "from nn.layers import Leaky_ReLU\n",
    "\n",
    "batch_size = 10\n",
    "in_features = 10\n",
    "input = np.random.uniform(size=(batch_size, in_features))\n",
    "out_grad = np.random.uniform(size=(batch_size, in_features))\n",
    "leaky_relu = Leaky_ReLU()\n",
    "\n",
    "check_grads_layer(leaky_relu, input, out_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ya6LY7uVALW-"
   },
   "source": [
    "## Classification using a simple two layer perceptron\n",
    "\n",
    "The following code trains a simple classifier defined in models/simple_classifier.py.\n",
    "\n",
    "Your task is to tune the architecture and the hyper-parameters to improve the performance (validation accuracy) of the model.\n",
    "\n",
    "You should submit this notebook with the best performed model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zfwwf8vSALXA",
    "outputId": "3ff45e2d-f21b-48a9-9daf-ada233e29416"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.dataloader import *\n",
    "from models.simple_classifier import simple_classifier\n",
    "from nn.optimizers import *\n",
    "        \n",
    "# hyper-parameters\n",
    "batch = 32\n",
    "log_freq = 50 # log-printing frequency\n",
    "batches_of_epoch = len(X_train) // batch\n",
    "epochs = 20\n",
    "\n",
    "data_loader = batch_loader(X_train, y_train, batch, shuffle=False)\n",
    "model = simple_classifier(n_in=784, n_out1=120, n_out2=2)\n",
    "opt = Adamax() #, momentum=0.8)\n",
    "\n",
    "metrics = [] # to store intermediate results during optimization\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"==> epoch {i+1}\")\n",
    "    \n",
    "    sum_train_acc, sum_train_loss = 0, 0\n",
    "\n",
    "    for itr in range(batches_of_epoch):\n",
    "        # get batch of data\n",
    "        X_train_b, y_train_b = next(data_loader)\n",
    "\n",
    "        train_acc, train_loss = model.forward(X_train_b, y_train_b)\n",
    "        grads = model.backward(X_train_b, y_train_b)\n",
    "        params,grads =  model.get_params()\n",
    "        new_params = opt.update(params, grads, itr)\n",
    "        model.update(new_params)\n",
    "\n",
    "        sum_train_acc += train_acc\n",
    "        sum_train_loss += train_loss\n",
    "\n",
    "        if itr % log_freq == 0:\n",
    "            print(\"\\t iter %d \\t train acc = %.2f%%, train loss = %.4f\" %(itr, train_acc, train_loss))\n",
    "\n",
    "    avg_train_acc, avg_train_loss = sum_train_acc / batches_of_epoch, sum_train_loss / batches_of_epoch\n",
    "\n",
    "    test_acc, test_loss = model.forward(X_test, y_test)\n",
    "    print(\"\\t avg train acc = %.2f%%, avg train loss = %.4f | test acc = %.2f%%, test loss = %.4f\" \n",
    "          % (avg_train_acc, avg_train_loss, test_acc, test_loss))\n",
    "        \n",
    "    metrics += [[avg_train_acc, avg_train_loss, test_acc, test_loss]] # to store intermediate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAqh2utyALXF"
   },
   "source": [
    "# Marking Scheme\n",
    "\n",
    "Marking scheme is shown below(15 marks in total):\n",
    "-  3 marks for `Adamax` update function\n",
    "-  2 marks for `linear` forward function, 2 marks for `linear` backward function\n",
    "-  1 marks for `leaky relu` forward function, 2 mark for `leaky relu` backward function\n",
    "-  3 marks for simple_classfier implementation and model tuning\n",
    "-  1 mark for code style\n",
    "-  1 mark for submission format\n",
    "\n",
    "\n",
    "We will run multiple test cases to check the correctness of your implementation. You may not get the full marks even if you pass the tests in this notebook as we have a few other test cases for each task, which are not included in this notebook.\n",
    "\n",
    "The submitted main.ipynb should include the running results of all cells. The model tuning part will be evaluated based on the tuning result (i..e, the printed metrics).\n",
    "\n",
    "As for submission format, please follow below submission instructions.\n",
    "\n",
    "**DO NOT** use external libraries like Tensorflow, keras and Pytorch in your implementation. **DO NOT** copy the code from the internet, e.g. github. We have offered all materials that you can refer to in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxBe54LkALXG"
   },
   "source": [
    "# Final submission instructions\n",
    "Please submit the following:\n",
    "\n",
    "1) Your codes in a folder named `codes`, and keep the structure of all files in this folder the same as what we have provided. \n",
    "\n",
    "**ASSIGNMENT DEADLINE: Week 7, 29 Sep 17:00, 15% off per day late (17:01 is the start of one day)**\n",
    "\n",
    "Do not include the `data` folder. Please zip the following folders under a folder named with your student number: eg. `a0123456g.zip` and submit the zipped folder to LumiNUS/Files/Assignment Submission. If unzip the file, the structure should be like this:\n",
    "\n",
    "```bash\n",
    "a0123456g/\n",
    "    codes/\n",
    "        models/\n",
    "            ...\n",
    "        nn/\n",
    "            ...\n",
    "        utils/\n",
    "            ...\n",
    "        main.ipynb\n",
    "        README.MD\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvTWm9GbALXI",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkI1Zl1-ALXN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@deathbeds/jupyterlab-fonts": {
   "fontLicenses": {
    "Anonymous Pro Bold": {
     "holders": [
      "Copyright (c) 2009, Mark Simonson (http://www.ms-studio.com, mark@marksimonson.com), with Reserved Font Name Anonymous Pro Minus."
     ],
     "name": "SIL Open Font License 1.1",
     "spdx": "OFL-1.1",
     "text": "Copyright (c) 2009, Mark Simonson (http://www.ms-studio.com, mark@marksimonson.com),\nwith Reserved Font Name Anonymous Pro Minus.\n\nThis Font Software is licensed under the SIL Open Font License, Version 1.1.\nThis license is copied below, and is also available with a FAQ at:\nhttp://scripts.sil.org/OFL\n\n\n-----------------------------------------------------------\nSIL OPEN FONT LICENSE Version 1.1 - 26 February 2007\n-----------------------------------------------------------\n\nPREAMBLE\nThe goals of the Open Font License (OFL) are to stimulate worldwide\ndevelopment of collaborative font projects, to support the font creation\nefforts of academic and linguistic communities, and to provide a free and\nopen framework in which fonts may be shared and improved in partnership\nwith others.\n\nThe OFL allows the licensed fonts to be used, studied, modified and\nredistributed freely as long as they are not sold by themselves. The\nfonts, including any derivative works, can be bundled, embedded,\nredistributed and/or sold with any software provided that any reserved\nnames are not used by derivative works. The fonts and derivatives,\nhowever, cannot be released under any other type of license. The\nrequirement for fonts to remain under this license does not apply\nto any document created using the fonts or their derivatives.\n\nDEFINITIONS\n\"Font Software\" refers to the set of files released by the Copyright\nHolder(s) under this license and clearly marked as such. This may\ninclude source files, build scripts and documentation.\n\n\"Reserved Font Name\" refers to any names specified as such after the\ncopyright statement(s).\n\n\"Original Version\" refers to the collection of Font Software components as\ndistributed by the Copyright Holder(s).\n\n\"Modified Version\" refers to any derivative made by adding to, deleting,\nor substituting -- in part or in whole -- any of the components of the\nOriginal Version, by changing formats or by porting the Font Software to a\nnew environment.\n\n\"Author\" refers to any designer, engineer, programmer, technical\nwriter or other person who contributed to the Font Software.\n\nPERMISSION & CONDITIONS\nPermission is hereby granted, free of charge, to any person obtaining\na copy of the Font Software, to use, study, copy, merge, embed, modify,\nredistribute, and sell modified and unmodified copies of the Font\nSoftware, subject to the following conditions:\n\n1) Neither the Font Software nor any of its individual components,\nin Original or Modified Versions, may be sold by itself.\n\n2) Original or Modified Versions of the Font Software may be bundled,\nredistributed and/or sold with any software, provided that each copy\ncontains the above copyright notice and this license. These can be\nincluded either as stand-alone text files, human-readable headers or\nin the appropriate machine-readable metadata fields within text or\nbinary files as long as those fields can be easily viewed by the user.\n\n3) No Modified Version of the Font Software may use the Reserved Font\nName(s) unless explicit written permission is granted by the corresponding\nCopyright Holder. This restriction only applies to the primary font name as\npresented to the users.\n\n4) The name(s) of the Copyright Holder(s) or the Author(s) of the Font\nSoftware shall not be used to promote, endorse or advertise any\nModified Version, except to acknowledge the contribution(s) of the\nCopyright Holder(s) and the Author(s) or with their explicit written\npermission.\n\n5) The Font Software, modified or unmodified, in part or in whole,\nmust be distributed entirely under this license, and must not be\ndistributed under any other license. The requirement for fonts to\nremain under this license does not apply to any document created\nusing the Font Software.\n\nTERMINATION\nThis license becomes null and void if any of the above conditions are\nnot met.\n\nDISCLAIMER\nTHE FONT SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT\nOF COPYRIGHT, PATENT, TRADEMARK, OR OTHER RIGHT. IN NO EVENT SHALL THE\nCOPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nINCLUDING ANY GENERAL, SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL\nDAMAGES, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF THE USE OR INABILITY TO USE THE FONT SOFTWARE OR FROM\nOTHER DEALINGS IN THE FONT SOFTWARE.\n"
    }
   },
   "fonts": {
    "Anonymous Pro Bold": [
     {
      "fontFamily": "'Anonymous Pro Bold'",
      "src": "url('data:font/woff2;charset=utf-8;base64,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') format('woff2')"
     }
    ]
   },
   "styles": {
    ":root": {
     "--jp-code-font-family": "'Anonymous Pro Bold'",
     "--jp-code-font-size": "16px",
     "--jp-content-font-size1": "16px"
    }
   }
  },
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
